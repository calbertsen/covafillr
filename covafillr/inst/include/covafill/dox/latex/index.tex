covafill is a C++ template library for local polynomial regression of covariates in state-\/space models. The covafill library is based on the \href{http://http://eigen.tuxfamily.org}{\tt Eigen} library for linear algebra, and includes several modules\+:


\begin{DoxyItemize}
\item The \hyperlink{group__core}{Core module} which provides the base functionality for local polynomial regression
\item The \hyperlink{group__tree}{Tree module} which provides a search tree approximation to local polynomial regression
\item The \hyperlink{group__interpolate}{Interpolate module} which provides classes for cubic interpolation in 1-\/3 dimensions. This module is only inteded for internal use.
\item The \hyperlink{group__jags}{J\+A\+G\+S module}, which provides a module for using covafill with \href{http://mcmc-jags.sourceforge.net/}{\tt J\+A\+G\+S}
\item The \hyperlink{group__tmb}{T\+M\+B module} which provides functionality to use covafill with \href{http://tmb-project.org}{\tt T\+M\+B}.
\end{DoxyItemize}

\subsubsection*{The Core module}

The Core module provides the class \hyperlink{classcovafill}{covafill} for local polynomial regression.

\paragraph*{Local polynomial regression}

For simplicity, consider the univariate model

\[ y_i = g(x_i) + \epsilon_i \]

where $g:\mathbb{R}\mapsto\mathbb{R}$ is a smooth function and $ \epsilon_i\sim N(0,\sigma^2)$. To do local polynomial regression of $g$ at $x_0$, we do a Taylor expansion of order $p$,

\[ g(x) \approx g(x_0) + g^{(1)}(x_0)(x-x_0) + \frac{1}{2!} g^{(2)}(x_0)(x-x_0)^2 + \cdots + \frac{1}{p!} g^{(p)}(x_0)(x-x_0)^p \] Substituting into the original model, \[ y_i = g(x_0) + g^{(1)}(x_0)(x-x_0) + \frac{1}{2!} g^{(2)}(x_0)(x-x_0)^2 + \cdots + \frac{1}{p!} g^{(p)}(x_0)(x-x_0)^p + \epsilon_i \] we obtain a linear model with coefficients $ \mathbf{\theta} = (g(x_0), g^{(1)}(x_0), g^{(2)}(x_0), \ldots, g^{(p)}(x_0))^T $, obervations $ \mathbf{Y} = (y_1, y_2, \ldots, y_n)^T $, and the design matrix \[ \mathbf{X} = \left(\begin{array}{ccccc} 1 & (x_1-x_0) & \frac{1}{2!} (x_1-x_0)^2 & \cdots & \frac{1}{p!} (x_1-x_0)^p \\ 1 & (x_2-x_0) & \frac{1}{2!} (x_2-x_0)^2 & \cdots & \frac{1}{p!} (x_2-x_0)^p \\ \vdots & \vdots & \vdots & & \vdots \\ 1 & (x_n-x_0) & \frac{1}{2!} (x_n-x_0)^2 & \cdots & \frac{1}{p!} (x_n-x_0)^p \end{array}\right) \] As we are interested in a local estimate, observations are weighed by their distance to $x_0$. The weights form the diagonal matrix $\mathbf{W}$ with \[ w_{ii} = \det(H^{-1}) \left(1 - \| H^{-1} \cdot ( x_i - x_0) \| ^ 2 \right) \vee 0 \]

Now the estimates are obtained by \[ \hat{\mathbf{\theta}} = (\mathbf{X}^T\mathbf{W}\mathbf{X})^{-1} \mathbf{X}^T\mathbf{W}\mathbf{Y} \] giving both the estimated function value at $ x_0 $ and estimates of the first $ p $ derivatives.

A covariance matrix for the estimates can be calculated by

\[ V(\hat{\mathbf{\theta}}) = (\mathbf{X}^T\mathbf{W}\mathbf{X})^{-1} (\mathbf{R}^T \mathbf{W} \mathbf{R}) (N-q)^{-1} \]

where $ N $ is the number of observations with non-\/negative weights, $ q $ is the number of regressors, and

\[ \mathbf{R} = \mathbf{Y}-\mathbf{X}\hat{\mathbf{\theta}} \]

Note that it is not necessary to have a properly normalized kernel function as the normalizing constant vanishes in the calculation of both the estimates and covariance matrix.

\subsubsection*{The Tree module}

The Tree module contains the covatree class for a search tree approximation to local polynomial regression. The covatree class builds a simple search tree by splitting the data in the coordinate with the highest variance. The split is performed at the mean of the coordinates. At terminal notes, the class does local polynomial regression at the corners of the bounding box of the coordinates related to the note and calculates the necessary coefficients to do cubic interpolation between the corners.

\subsubsection*{The J\+A\+G\+S and T\+M\+B modules}

The J\+A\+G\+S and T\+M\+B modules provide functionality to use covafill within these tools. The J\+A\+G\+S module provides a J\+A\+G\+S Module including a function, covafill, to call in a J\+A\+G\+S model (See example below). The T\+M\+B module include functions to evaluate a covafill or covatree object from a T\+M\+B model such that the estimated derivatives are used in the automatic differentiation (See example below).

\paragraph*{J\+A\+G\+S example}


\begin{DoxyCode}
model \{
      cf <- \hyperlink{classcovafill}{covafill}(x,obsC,obs,h,2.0)
      sigma ~ dunif(0,100)
      tau <- pow(sigma, -2)
      for(i in 1:N) \{
            y[i] ~ dnorm(cf[i],tau)
        \}
\}
\end{DoxyCode}


\paragraph*{T\+M\+B example}


\begin{DoxyCode}
\textcolor{preprocessor}{#include <TMB.hpp>}
\textcolor{preprocessor}{#include <covafill/TMB>}

\textcolor{keyword}{template}<\textcolor{keyword}{class} Type>
Type objective\_function<Type>::operator() ()
\{
  DATA\_MATRIX(obs);
  DATA\_MATRIX(coord);
  DATA\_VECTOR(covObs);
  DATA\_INTEGER(p);
  DATA\_VECTOR(h);

  PARAMETER(logObsSd);
  PARAMETER(logObsTSd);
  PARAMETER(logStatSd);
  PARAMETER\_MATRIX(x);

  Type nll = 0.0;
  \hyperlink{classcovafill}{covafill<Type>} cf(coord,covObs,h,p);

  \textcolor{comment}{// Contribution from states}
  \textcolor{keywordflow}{for}(\textcolor{keywordtype}{int} i = 1; i < x.cols(); ++i)\{
    nll -= dnorm(x(0,i), x(0,i-1), exp(logStatSd),\textcolor{keyword}{true});
    nll -= dnorm(x(1,i), x(1,i-1), exp(logStatSd),\textcolor{keyword}{true});
  \}

  \textcolor{comment}{// contribution from observations}
  \textcolor{keywordflow}{for}(\textcolor{keywordtype}{int} i = 0; i < obs.cols(); ++i)\{
    nll -= dnorm(obs(0,i), x(0,i), exp(logObsSd),\textcolor{keyword}{true});
    nll -= dnorm(obs(1,i), x(1,i), exp(logObsSd),\textcolor{keyword}{true});
    vector<Type> tmp = x.col(i);
    Type val = \hyperlink{group__tmb_ga0862de227e5abdeba2394525467bafe4}{evalFill}((CppAD::vector<Type>)tmp, cf)[0];
    nll -= dnorm(obs(2,i), val, exp(logObsTSd),\textcolor{keyword}{true});
  \}


  \textcolor{keywordflow}{return} nll;
\}
\end{DoxyCode}
 